# üåå Cosmos-Lucid-Dreamer

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Paper](https://img.shields.io/badge/Paper-Read%20Technical%20Report-brightgreen.svg)](docs/TECHNICAL_REPORT.md)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-red.svg)](https://pytorch.org/)

> **"Physics is observable only when you look at history."**

**ResNet-Rollout**: A simple, fast, and highly accurate latent world model for video prediction.

**Performance**: 71.08 px RMSE | 99% Detection | ~1M Parameters | 15 Minutes Training

---

## üèÜ Key Results

We achieved a **56% reduction in error** compared to high-capacity baselines by using a simple ResNet with a 4-frame context window.

| Metric | ResNet-Rollout (Ours) | Previous Best | Improvement |
|--------|-----------------------|---------------|-------------|
| **RMSE** | **71.08 pixels** | 165.20 px | **56% Better** |
| **Detection Rate** | **99%** | 100% | **Parity** |
| **Model Size** | **~1.02 M** | ~84 M | **84√ó Smaller** |
| **Training Time** | **15 mins** | 45 mins | **3√ó Faster** |

---

## üî¨ The Science: From "Fever Dream" to "Lucid Dream"

**ResNet-Rollout** is a latent world model that predicts future states in the [Cosmos tokenizer](https://github.com/NVIDIA/Cosmos-Tokenizer) latent space. It combines three key innovations:

1. **Simple ResNet Architecture** (~1M params, 4 residual blocks, no skip connections)
2. **4-Frame Context Window** (provides velocity information, eliminates blur)
3. **Teacher-Less Rollout Training** (enforces temporal consistency)

### Why It Works: The "Lucid Dream" Principle

Single-frame models suffer from **Posterior Collapse**‚Äîwithout velocity information, they must average over all possible futures, resulting in blur. A 4-frame context window provides observable velocity, disambiguating future predictions and enabling sharp, deterministic output.

![Lucid Dream Concept](docs/figure_1_concept.png)

---

## üöÄ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/Pouria-007/Cosmos-Lucid-Dreamer.git
cd Cosmos-Lucid-Dreamer

# Install dependencies
pip install -r requirements.txt
```

### 2. Prerequisites

You need:
- **Tokenized latent data** (`latent.pt`): Use [Cosmos-Tokenizer](https://github.com/NVIDIA/Cosmos-Tokenizer) to encode your video frames
- **Normalization statistics** (`latent_stats.pt`): Mean and std of latent space
- **GPU**: NVIDIA GPU with CUDA support (tested on RTX 5090)

### 3. Training (15 Minutes)

```bash
cd src
python train.py --epochs 500 --batch_size 32 --lr 1e-4 \
  --context_window 4 \
  --rollout_steps 4 \
  --output_dir ../checkpoints
```

### 4. Dreaming (Inference)

```bash
cd src
python dream.py --checkpoint ../checkpoints/resnet_rollout_best.pt \
  --output_path ../results/dreamed_latents.pt
```

### 5. Analysis

```bash
cd src
python analyze.py --ground_truth ../cosmic_data/frames \
  --dreamed ../results/decoded_frames \
  --output_figure ../results/tracking_analysis.png
```

---

## üìä Architecture

We use a surprisingly simple architecture to prove that **data structure matters more than model depth**.

### Model Specification

```python
Input:  [Batch, 64, 1, 64, 64]  # Stack of 4 Latent Frames (16ch each)
Output: [Batch, 16, 1, 64, 64]  # Next Latent Frame

Architecture:
  - Conv3d(64 ‚Üí 64) + GroupNorm + SiLU
  - 4√ó ResidualBlock3D(64)
  - Conv3d(64 ‚Üí 16)

Parameters: ~1,024,720 (~1M)
```

![Architecture](docs/figure_2_architecture.png)

---

## üìà Performance Comparison

The "Inverted-U" curve shows that **over-parameterization actually hurts performance** in autoregressive tasks due to stability issues.

| Model Strategy | RMSE (px) | Detection | Status |
|----------------|-----------|-----------|--------|
| **ResNet-Rollout (Ours)** | **71.08** | **99%** | **‚úÖ Winner** |
| Probabilistic Plus | 77.81 | 100% | ‚ö†Ô∏è Jittery |
| Sharp-Shooter (GDL) | 108.0 | 59% | ‚ùå Unstable |
| Rollout U-Net | 165.2 | 100% | ‚ùå Blurry |
| Hero Model (394M) | N/A | 30% | ‚ùå Failed |

![Results](docs/figure_3_results.png)

---

## üé¨ Demo Video

**Left**: Ground Truth | **Center**: Probabilistic (Jitter) | **Right**: ResNet-Rollout (Sharp)

[Watch Comparison Video](docs/demo_video.mp4)

---

## üìÅ Repository Structure

```
Cosmos-Lucid-Dreamer/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ train.py          # Training script (Teacher-less Rollout)
‚îÇ   ‚îú‚îÄ‚îÄ dream.py          # Inference script
‚îÇ   ‚îú‚îÄ‚îÄ analyze.py        # RMSE and Centroid tracking analysis
‚îÇ   ‚îú‚îÄ‚îÄ modules.py        # ResNet Architecture
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py        # Context-Aware Dataloader
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ generate_paper_figures.py  # Reproduction for paper figures
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îî‚îÄ‚îÄ resnet_rollout_best.pt     # Trained model (71.08 px RMSE)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ TECHNICAL_REPORT.md        # Detailed technical report
‚îÇ   ‚îú‚îÄ‚îÄ figure_1_concept.png       # Concept diagram
‚îÇ   ‚îú‚îÄ‚îÄ figure_2_architecture.png  # Architecture diagram
‚îÇ   ‚îú‚îÄ‚îÄ figure_3_results.png       # Performance comparison
‚îÇ   ‚îî‚îÄ‚îÄ demo_video.mp4             # Comparison video
‚îú‚îÄ‚îÄ results/                       # Output directory
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üî¨ Key Insights

### 1. Small Models Beat Large Models

- **1M parameters** outperforms **84M parameters**
- Less overfitting ‚Üí better generalization
- Faster training, easier deployment

### 2. Context > Architecture

- **4-frame context window** provides velocity information
- More important than skip connections (U-Net)
- Disambiguates future predictions ‚Üí eliminates blur

### 3. Motion-Weighted Loss is Essential

- **10√ó weight on moving pixels**
- Forces model to track objects (not background)
- **99% detection rate**

### 4. Teacher-Less Rollout Enforces Consistency

- Predictions feed back as input
- Prevents error accumulation
- Smooth, stable trajectories

---

## üìù Citation

If you use this code or the "Lucid Dream" concept in your research, please cite:

```bibtex
@article{cosmos_lucid_dreamer_2025,
  title={Cosmic Dreams: Lucid Dreaming in Latent Space via Temporal Context Windows},
  author={Javaheri, Pouria},
  journal={TechRxiv},
  year={2025},
  doi={10.36227/techrxiv.176784313.32092392/v1},
  url={https://www.techrxiv.org/users/1001132/articles/1373525}
}
```

**Publication**: [TechRxiv Article](https://www.techrxiv.org/users/1001132/articles/1373525-cosmic-dreams-lucid-dreaming-in-latent-space-via-temporal-context-windows)

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [NVIDIA Cosmos Tokenizer](https://github.com/NVIDIA/Cosmos-Tokenizer) for the latent space encoder/decoder
- [World Models (Ha & Schmidhuber, 2018)](https://worldmodels.github.io/) for inspiration
- The PyTorch team for excellent tools

---

**Status**: ‚úÖ Production Ready | üì¶ Tested on NVIDIA RTX 5090
